Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Collobert2009,
abstract = {Deep learning has emerged as a new area of machine learning research. It tries to mimic the human brain, which is capable of processing and learning from the complex input data and solving different kinds of complicated tasks well. It has been successfully applied to several fields such as images, sounds, text and motion. The techniques developed from deep learning research have already been impacting the research of natural language process. This paper reviews the recent research on deep learning, its applications and recent development in natural language processing.},
author = {Collobert, Ronan and Weston, Jason},
doi = {10.1.1.678.8129},
file = {:D$\backslash$:/Rosa/PhD/Deep Learning for Procedural NLP/Deep Learning for Natural Language Processing.pdf:pdf},
journal = {Slides},
pages = {1--113},
title = {{Deep Learning for Natural Language Processing}},
url = {https://www.eecis.udel.edu/{~}vijay/fall13/snlp/lit-survey/DeepLearning.pdf{\%}0Apapers2://publication/uuid/BFC64DC3-2F4D-495A-ABF2-AD7A458FBF4B},
year = {2009}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1701.07274},
file = {:D$\backslash$:/Rosa/PhD/Deep Learning for Procedural NLP/DEEP REINFORCEMENT LEARNING- AN OVERVIEW.pdf:pdf},
issn = {1701.07274},
pages = {1--70},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@article{Johnson-Laird1977,
abstract = {The aim of this paper is to present an outline of a theory of semantics based on the analogy between natural and computer programming languages. A unified model of the comprehension and production of sentences is described in order to illustrate the central "compile and execute" metaphor underlying prodecural semantics. The role of general knowledge within the lexicon, and the mechanism mediating selectional restrictions, are re-analyzed in the light of the procedural theory. {\textcopyright} 1977.},
author = {Johnson-Laird, Philip N.},
doi = {10.1016/0010-0277(77)90001-4},
file = {:D$\backslash$:/Rosa/PhD/Deep Learning for Procedural NLP/Procedural semantics.pdf:pdf},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {189--214},
title = {{Procedural semantics}},
volume = {5},
year = {1977}
}
@article{Deng2014,
abstract = {Deep Learning: Methods and Applications},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Deng, Li and Yu, Dong},
doi = {10.1561/2000000039},
eprint = {1309.1501},
file = {:C$\backslash$:/Users/45109/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng, Yu - 2014 - Deep Learning Methods and Applications.pdf:pdf},
isbn = {9781405161251},
issn = {1932-8346},
journal = {Foundations and Trends{\textregistered} in Signal Processing},
keywords = {Architectures for IR,Audio signal processing,Information Retrieval,Signal Processing,Speech and spoken language processing,Statistical/machine learning},
number = {3-4},
pages = {197--387},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
url = {http://nowpublishers.com/articles/foundations-and-trends-in-signal-processing/SIG-039},
volume = {7},
year = {2014}
}
@article{Hamada2000,
abstract = {We propose a method to create process flow graphs automatically from textbooks for cooking programs. This is realized by understanding context by narrowing down the domain to cooking, and making use of domain specific constraints and knowledge. Since it is relatively easy to extract significant keywords from cooking procedures, we create a domain specific dictionary by statistical methods, and propose a structural analysis method using the dictionary. In order to evaluate the ability of the proposed method, we applied the method to actual procedures as an experiment, which showed effective results. The same experiment was also performed on a different program, which showed lower accuracy but also showed realistic results.},
author = {Hamada, Reiko and Ide, Ichiro and Sakai, Shuichi and Tanaka, Hidehiko},
doi = {10.1145/355214.355237},
isbn = {1581133006},
journal = {Proceedings of the fifth international workshop on on Information retrieval with Asian languages - IRAL '00},
keywords = {cookbooks,domain specific dictionary,preparation steps,structural analysis},
number = {xx},
pages = {157--164},
title = {{Structural analysis of cooking preparation steps in Japanese}},
url = {http://portal.acm.org/citation.cfm?doid=355214.355237},
year = {2000}
}
@article{Deselaers2009,
abstract = {In this paper we present a novel transliteration technique which is based on deep belief networks. Common approaches use finite state machines or other methods similar to conventional machine translation. Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems. We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task.},
author = {Deselaers, Thomas and Hasan, Sasa and Bender, Oliver and Ney, Hermann},
doi = {10.3115/1626431.1626476},
file = {:D$\backslash$:/Rosa/PhD/Deep Learning for Procedural NLP/A Deep Learning Approach to Machine Transliteration.pdf:pdf},
journal = {Computational Linguistics},
keywords = {learning,natural language processing,statistics {\&} optimisation},
number = {March},
pages = {233--241},
title = {{A Deep Learning Approach to Machine Transliteration}},
url = {http://eprints.pascal-network.org/archive/00005734/},
year = {2009}
}
@misc{Young2017,
abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
archivePrefix = {arXiv},
arxivId = {1708.02709},
author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
booktitle = {arXiv},
eprint = {1708.02709},
file = {:D$\backslash$:/Rosa/PhD/Deep Learning for Procedural NLP/Recent Trends in Deep Learning Based.pdf:pdf},
keywords = {and engineering,ek laboratories,nanyang technological university,school of computer science,singapore},
pages = {1--22},
title = {{Recent Trends in Deep Learning Based Natural Language Processing}},
url = {http://arxiv.org/abs/1708.02709},
year = {2017}
}
@article{Sutton1998,
abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {5},
pages = {1054--1054},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction}},
url = {http://ieeexplore.ieee.org/document/712192/},
volume = {9},
year = {1998}
}
@article{Arulkumaran2017a,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:D$\backslash$:/Rosa/PhD/Deep Learning for Procedural NLP/A Brief Survey of Deep Reinforcement Learning.pdf:pdf},
isbn = {9781424469178},
issn = {1701.07274},
pages = {1--16},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@misc{DrDBollegala,
author = {{Danushka Bollegala}},
title = {{Deep Learning for Procedural Natural Language Understanding at University of Liverpool on FindAPhD.com}},
url = {https://www.findaphd.com/search/projectdetails.aspx?PJID=89096},
urldate = {2017-11-29}
}
@article{Maeta2015,
author = {Maeta, Hirokuni and Sasada, Tetsuro and Mori, Shinsuke},
file = {:D$\backslash$:/Rosa/PhD/Deep Learning for Procedural NLP/A Framework for Procedural Text Understanding.pdf:pdf},
journal = {Proceedings of the 14th International Conference on Parsing Technologies},
pages = {50--60},
title = {{A Framework for Procedural Text Understanding}},
url = {http://plata.ar.media.kyoto-u.ac.jp/mori/research/public/maeta-IWPT15.pdf},
year = {2015}
}
